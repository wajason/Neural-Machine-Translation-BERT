# ğŸš€ BERT Transfer Learning for Neural Machine Translation

This repository explores **transfer learning with BERT** applied to **Neural Machine Translation (NMT)** using Recurrent Neural Networks (RNNs).  
It is based on **Stanford's CS224N Assignment 3**, with additional notes and structured code for learning and reference purposes.

---

## ğŸ“‚ What's Inside
- `Neural_Machine_Translation_with_RNNs.ipynb` â€” the complete Jupyter Notebook implementation.

---

## âœ¨ Highlights
- Fine-tuning **BERT** for sequence classification and translation tasks.
- Exploring **transfer learning** in the context of **Neural Machine Translation**.
- Step-by-step implementation in PyTorch.
- Includes evaluation with BLEU score.

---

## âš™ï¸ Requirements
Install dependencies with:
```bash
pip install -r requirements.txt
```

## ğŸ“Š Results
Achieved competitive BLEU scores as required in CS224N Assignment 3.

Demonstrated the power of BERT for transfer learning in sequence-to-sequence tasks.

## ğŸ“– References
Stanford CS224N: Natural Language Processing with Deep Learning

Vaswani et al. (2017). Attention is All You Need.

Devlin et al. (2018). BERT: Pre-training of Deep Bidirectional Transformers for Language Understanding.

## âš ï¸ Disclaimer
This repository is created solely for personal study, learning, and reference purposes.
It is not affiliated with or endorsed by Stanford University.
If you are currently taking CS224N, do not use this repository as your assignment submission â€” it is intended only as a study record.
Please respect academic integrity.

â­ Contribute & Support
Let's grow the open-source NLP learning community together ğŸš€